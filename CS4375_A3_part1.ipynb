{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOU3VPajVbJ8vAYlHQ7rOb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharveshSp04/CS-4375-Assignments/blob/main/CS4375_A3_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbLM2fAsIqkI",
        "outputId": "4dd950fa-3a78-49ae-e90a-d141b1ef3129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CS 4375 - ASSIGNMENT 3 - PART I SOLUTIONS\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "QUESTION 1: Bagging Error Analysis\n",
            "==================================================\n",
            "\n",
            "PROOF SUMMARY:\n",
            "E_agg = E[(1/M * Σ h_i(x) - f(x))²]\n",
            "     = E[(1/M * Σ (f(x) - ε_i(x)) - f(x))²]\n",
            "     = E[(-1/M * Σ ε_i(x))²]\n",
            "     = 1/M² * E[(Σ ε_i(x))²]\n",
            "     = 1/M² * [Σ E[ε_i²] + ΣΣ E[ε_iε_j]]  (expand square)\n",
            "     = 1/M² * Σ E[ε_i²]  (since E[ε_iε_j] = 0 for i ≠ j)\n",
            "     = 1/M * E_avg\n",
            "\n",
            "Key Insight: Uncorrelated errors eliminate cross-terms, reducing error by factor M.\n",
            "\n",
            "\n",
            "==================================================\n",
            "QUESTION 2: Jensen's Inequality Application\n",
            "==================================================\n",
            "\n",
            "PROOF SUMMARY:\n",
            "Using Jensen's inequality for convex function f(x) = x²:\n",
            "[(1/M) * Σ ε_i]² ≤ (1/M) * Σ ε_i²\n",
            "\n",
            "Take expectation:\n",
            "E[(1/M * Σ ε_i)²] ≤ E[(1/M) * Σ ε_i²]\n",
            "E_agg ≤ (1/M) * Σ E[ε_i²] = E_avg\n",
            "\n",
            "Key Insight: Even with correlated errors, aggregated model is never worse than average individual model.\n",
            "\n",
            "\n",
            "==================================================\n",
            "QUESTION 3: AdaBoost Training Error Bound\n",
            "==================================================\n",
            "\n",
            "PROOF SUMMARY:\n",
            "1. Final weight: D_{T+1}(i) = (1/N) * exp(-y_i Σ α_t h_t(x_i)) / Π Z_t\n",
            "2. Error indicator: [H(x_i) ≠ y_i] ≤ exp(-y_i Σ α_t h_t(x_i))\n",
            "3. Training error ≤ (1/N) * Σ exp(-y_i Σ α_t h_t(x_i)) = Π Z_t\n",
            "4. Z_t = (1-ε_t)e^{-α_t} + ε_t e^{α_t} = 2√(ε_t(1-ε_t)) = √(1-4γ_t²)\n",
            "5. Bound: √(1-4γ_t²) ≤ exp(-2γ_t²)\n",
            "6. Therefore: Training error ≤ exp(-2 Σ γ_t²)\n",
            "\n",
            "Key Insight: AdaBoost training error decreases exponentially with number of weak learners.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "END OF PART I SOLUTIONS\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# CS 4375 - Assignment 3 - Part I\n",
        "# Theoretical Solutions File\n",
        "\n",
        "def display_solutions():\n",
        "    \"\"\"Display the theoretical solutions for Part I\"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CS 4375 - ASSIGNMENT 3 - PART I SOLUTIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Question 1\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QUESTION 1: Bagging Error Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\"\"\n",
        "PROOF SUMMARY:\n",
        "E_agg = E[(1/M * Σ h_i(x) - f(x))²]\n",
        "     = E[(1/M * Σ (f(x) - ε_i(x)) - f(x))²]\n",
        "     = E[(-1/M * Σ ε_i(x))²]\n",
        "     = 1/M² * E[(Σ ε_i(x))²]\n",
        "     = 1/M² * [Σ E[ε_i²] + ΣΣ E[ε_iε_j]]  (expand square)\n",
        "     = 1/M² * Σ E[ε_i²]  (since E[ε_iε_j] = 0 for i ≠ j)\n",
        "     = 1/M * E_avg\n",
        "\n",
        "Key Insight: Uncorrelated errors eliminate cross-terms, reducing error by factor M.\n",
        "\"\"\")\n",
        "\n",
        "    # Question 2\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QUESTION 2: Jensen's Inequality Application\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\"\"\n",
        "PROOF SUMMARY:\n",
        "Using Jensen's inequality for convex function f(x) = x²:\n",
        "[(1/M) * Σ ε_i]² ≤ (1/M) * Σ ε_i²\n",
        "\n",
        "Take expectation:\n",
        "E[(1/M * Σ ε_i)²] ≤ E[(1/M) * Σ ε_i²]\n",
        "E_agg ≤ (1/M) * Σ E[ε_i²] = E_avg\n",
        "\n",
        "Key Insight: Even with correlated errors, aggregated model is never worse than average individual model.\n",
        "\"\"\")\n",
        "\n",
        "    # Question 3\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QUESTION 3: AdaBoost Training Error Bound\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\"\"\n",
        "PROOF SUMMARY:\n",
        "1. Final weight: D_{T+1}(i) = (1/N) * exp(-y_i Σ α_t h_t(x_i)) / Π Z_t\n",
        "2. Error indicator: [H(x_i) ≠ y_i] ≤ exp(-y_i Σ α_t h_t(x_i))\n",
        "3. Training error ≤ (1/N) * Σ exp(-y_i Σ α_t h_t(x_i)) = Π Z_t\n",
        "4. Z_t = (1-ε_t)e^{-α_t} + ε_t e^{α_t} = 2√(ε_t(1-ε_t)) = √(1-4γ_t²)\n",
        "5. Bound: √(1-4γ_t²) ≤ exp(-2γ_t²)\n",
        "6. Therefore: Training error ≤ exp(-2 Σ γ_t²)\n",
        "\n",
        "Key Insight: AdaBoost training error decreases exponentially with number of weak learners.\n",
        "\"\"\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"END OF PART I SOLUTIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    display_solutions()"
      ]
    }
  ]
}